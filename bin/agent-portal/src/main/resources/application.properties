spring.application.name=agent-portal

OPENAI_API_KEY=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

spring.ai.openai.api-key=${OPENAI_API_KEY}

model.init.ping=ollama.chat.*, open-ai.chat.*

task.chat-session.model-provider=ollama
task.embeddings.model-provider=ollama
task.test=Hello World!!

model.init.ollama.base-url=http://localhost:11434
model.init.ollama.chat.model-name=llama3.2
model.init.ollama.chat.generate-model-name=llama3.2
model.init.ollama.chat.stream-model-name=llama3.2
model.init.ollama.chat.vision-model-name=llama3.2
model.init.ollama.chat.embed-model-name=llama3.2

model.init.open-ai.api-key=${OPENAI_API_KEY}
model.init.open-ai.chat.model-name=gpt-4o-mini
model.init.open-ai.chat.vision-model-name=dall-e-3

# ---Model Parameters---

model.options.log-requests=true
model.options.log-responses=true
!! The maximum time allowed for the API call to complete.
model.options.timeout=PT60S
!! The maximum number of retries in case of API call failure.
model.options.max-retries=0
!! Sets the size of the context window used to generate the next token.
model.options.num-ctx=2048
!! The desired format for the generated output. TEXT or JSON with optional JSON Schema definition
model.options.response-format=TEXT
!! Sets the random number seed to use for generation. Setting this to a specific number will make the model generate the same text for the same prompt.
model.options.seed=-1
!! Maximum number of tokens to predict when generating text. (-1 = infinite generation, -2 = fill context)
model.options.num-predict=-1
!! Reduces the probability of generating nonsense. A higher value (e.g., 100) will give more diverse answers, while a lower value (e.g., 10) will be more conservative.
model.options.top-k=40
!! Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text.
model.options.top-p=0.9
!! The temperature of the model. Increasing the temperature will make the model answer more creatively.
model.options.temperature=0.8
!! Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient.
model.options.repeat-penalty=1.1
!! Sets the stop sequences to use. When this pattern is encountered the LLM will stop generating text and return. Multiple stop patterns may be set by specifying multiple separate stop parameters in a modelfile.
model.options.stop[0]=
